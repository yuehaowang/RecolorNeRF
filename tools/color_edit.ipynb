{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ec270-70c3-4117-b4f3-cfffde00f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import glob\n",
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "try:\n",
    "    import piplite\n",
    "    await piplite.install(['ipywidgets'])\n",
    "except ImportError:\n",
    "    pass\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286785cb-16c2-435c-83fb-4ebcad761d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.trainer import Trainer\n",
    "from engine.eval import evaluation_path\n",
    "from data import dataset_dict\n",
    "from utils.opt import config_parser\n",
    "from utils.vis import plot_palette_colors, visualize_depth_numpy, visualize_palette_components_numpy\n",
    "from utils.color import rgb2hex, hex2rgb\n",
    "from utils.ray import get_rays, ndc_rays_blender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313b8f6-5568-4cd1-82b3-3d92199348cf",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96867b9a-d9a8-4be3-8c6f-8926edbd2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_divider():\n",
    "    print()\n",
    "\n",
    "                    \n",
    "def render_one_view(test_dataset, tensorf, c2w, renderer, N_samples=-1,\n",
    "                    white_bg=False, ndc_ray=False, palette=None, device='cuda'):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    near_far = test_dataset.near_far\n",
    "\n",
    "    if palette is None and hasattr(tensorf, 'get_palette_array'):\n",
    "        palette = tensorf.get_palette_array().cpu()\n",
    "    \n",
    "    W, H = test_dataset.img_wh\n",
    "\n",
    "    c2w = torch.FloatTensor(c2w)\n",
    "    rays_o, rays_d = get_rays(test_dataset.directions, c2w)  # both (h*w, 3)\n",
    "    if ndc_ray:\n",
    "        rays_o, rays_d = ndc_rays_blender(H, W, test_dataset.focal[0], 1.0, rays_o, rays_d)\n",
    "    rays = torch.cat([rays_o, rays_d], 1)  # (h*w, 6)\n",
    "    \n",
    "    res = renderer(rays, tensorf, chunk=2048, N_samples=N_samples, palette=palette,\n",
    "                   ndc_ray=ndc_ray, white_bg=white_bg, device=device, ret_opaque_map=True)\n",
    "\n",
    "    rgb_map = res['rgb_map']\n",
    "    depth_map = res['depth_map']\n",
    "\n",
    "    rgb_map, depth_map = rgb_map.reshape(H, W, 3).cpu(), depth_map.reshape(H, W).cpu()\n",
    "\n",
    "    rgb_map = (rgb_map.numpy() * 255).astype('uint8')\n",
    "\n",
    "    depth_map, _ = visualize_depth_numpy(depth_map.numpy(), near_far)\n",
    "\n",
    "    is_vis_plt = (palette is not None) and ('opaque_map' in res)\n",
    "    plt_decomp = None\n",
    "    if is_vis_plt:\n",
    "        opaque = rearrange(res['opaque_map'], '(h w) c-> h w c', h=H, w=W).cpu()\n",
    "        plt_decomp = visualize_palette_components_numpy(opaque.numpy(), palette.numpy())\n",
    "        plt_decomp = (plt_decomp * 255).astype('uint8')\n",
    "    \n",
    "    return rgb_map, depth_map, plt_decomp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b7974a-b8d4-4a70-bb0d-52ae554bc71b",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad63c8-dfbc-46de-bec9-af83d611f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make paths accessible by this notebook\n",
    "path_redirect = [\n",
    "    # option name, path in the config, redirected path\n",
    "    ('palette_path', './data_palette', '../data_palette')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9317669-9b92-4495-a94c-8f80cbcaf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = '../logs/chair/'\n",
    "ckpt_path = None\n",
    "out_dir = os.path.join(run_dir, 'demo_out')\n",
    "\n",
    "print('Run dir:', run_dir)\n",
    "print('Demo output dir:', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608948d3-5eae-4fcb-a160-8d1c9d4172d8",
   "metadata": {},
   "source": [
    "## Load and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf3cd6-4c24-4ebd-bfa8-d10239c1d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read args\n",
    "parser = config_parser()\n",
    "config_path = os.path.join(run_dir, 'args.txt')\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        args, remainings = parser.parse_known_args(args=[], config_file_contents=f.read())\n",
    "        \n",
    "        # override ckpt path\n",
    "        if ckpt_path is not None:\n",
    "            setattr(args, 'ckpt', ckpt_path)\n",
    "        \n",
    "        # redirect path\n",
    "        for entry in path_redirect:\n",
    "            setattr(args, entry[0], getattr(args, entry[0]).replace(entry[1], entry[2]))\n",
    "\n",
    "        print('Args loaded:', args)\n",
    "else:\n",
    "    print(f'ERROR: cannot read args in {run_dir}.')\n",
    "print_divider()\n",
    "\n",
    "\n",
    "# Setup trainer\n",
    "print('Initializing trainer and model...')\n",
    "ckpt_dir = os.path.join(run_dir, 'checkpoints')\n",
    "tb_dir = os.path.join(run_dir, 'tensorboard')\n",
    "trainer = Trainer(args, run_dir, ckpt_dir, tb_dir)\n",
    "model = trainer.build_network()\n",
    "model.eval()\n",
    "print_divider()\n",
    "\n",
    "\n",
    "# Create downsampled dataset\n",
    "dataset = dataset_dict[args.dataset_name]\n",
    "ds_test_dataset = dataset(args.datadir, split='test', downsample=args.downsample_train * 2., is_stack=True)\n",
    "print('Downsampled dataset loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6bc71-0240-40d3-a0cd-4833ce762848",
   "metadata": {},
   "source": [
    "## Palette Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99edcadf-97b3-4d63-9701-38eed12d80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_prior = trainer.palette_prior.detach().cpu().numpy()\n",
    "palette = model.renderModule.palette.get_palette_array().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ce209-92e9-4b1c-831a-a9348a805297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initial palette prior:')\n",
    "plot_palette_colors(palette_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b197b4-3867-42da-8155-5e444c462ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimized palette:')\n",
    "new_palette = palette.clip(0, 1.)\n",
    "plot_palette_colors(new_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56695eda-d8a2-4e5c-84b1-3f990611fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pickers = []\n",
    "\n",
    "for i in range(palette.shape[0]):\n",
    "    color_picker = widgets.ColorPicker(concise=False, description=f'Color {i}', value=rgb2hex(new_palette[i]), disabled=False)\n",
    "    color_pickers.append(color_picker)\n",
    "\n",
    "box_layout = widgets.Layout(width='100%', grid_template_rows='auto', grid_template_columns='25% 25% 25% 25%')\n",
    "box_auto = widgets.GridBox(children=color_pickers, layout=box_layout)\n",
    "display(box_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074cca65-ed8e-4b70-8ae6-f993039b7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Palette for rendering:')\n",
    "\n",
    "new_palette = np.array([hex2rgb(cl_pk.value) for cl_pk in color_pickers]).astype(np.float32) / 255.\n",
    "plot_palette_colors(new_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf13f4-ba42-43b1-8d29-7eb398af7e63",
   "metadata": {},
   "source": [
    "## Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2bd11-5cd5-4113-8413-a129126a10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to change the rendering view\n",
    "render_cam_idx = 1\n",
    "\n",
    "c2w = ds_test_dataset.poses[render_cam_idx]\n",
    "white_bg = ds_test_dataset.white_bg\n",
    "ndc_ray = args.ndc_ray\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    rgb, depth, plt_decomps = render_one_view(ds_test_dataset, model, c2w, trainer.renderer, palette=torch.from_numpy(new_palette),\n",
    "                                              N_samples=-1, white_bg=white_bg, ndc_ray=ndc_ray, device=trainer.device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 16))\n",
    "axes[0].set_axis_off()\n",
    "axes[0].imshow(rgb)\n",
    "axes[1].set_axis_off()\n",
    "axes[1].imshow(depth)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(16, 8))\n",
    "axes.set_axis_off()\n",
    "axes.imshow(plt_decomps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9bd85-4b88-45c1-8102-eff07da5b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cells below to save this editing\n",
    "\n",
    "'''Modify this to name this editing'''\n",
    "edit_name = 'red_chair'\n",
    "\n",
    "assert edit_name\n",
    "\n",
    "out_fn = f'rgb_palette{\"_\" + edit_name if edit_name else \"\"}'\n",
    "out_path = os.path.join(out_dir, f'{out_fn}.npy')\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "if os.path.exists(out_path):\n",
    "    print('Error: file exists. Please specify another `edit_name`.')\n",
    "else:\n",
    "    np.save(out_path, new_palette)\n",
    "    print('Save palette to', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d105d-a714-4ae8-8785-e7701af1d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Choose between 'test' / 'path' '''\n",
    "cam_poses='test'\n",
    "\n",
    "save_dir = os.path.join(out_dir, f'render_{cam_poses}{\"_\" + edit_name if edit_name else \"\"}')\n",
    "\n",
    "if os.path.exists(save_dir):\n",
    "    print('Error: directory exists. Please specify another `edit_name`.')\n",
    "else:\n",
    "    c2ws = trainer.test_dataset.poses if cam_poses == 'test' else trainer.test_dataset.render_path\n",
    "    if cam_poses == 'test' and args.dataset_name == 'llff':\n",
    "        c2ws = c2ws[::8, ...]\n",
    "    white_bg = trainer.test_dataset.white_bg\n",
    "    ndc_ray = trainer.args.ndc_ray\n",
    "\n",
    "    print('Save renderings to', save_dir)\n",
    "    print('=== render path ======>', c2ws.shape)\n",
    "    with torch.no_grad():\n",
    "        evaluation_path(trainer.test_dataset, model, c2ws, trainer.renderer, save_dir, palette=torch.from_numpy(new_palette),\n",
    "                        N_samples=-1, white_bg=white_bg, ndc_ray=ndc_ray, save_video=True, device=trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45de877-cb57-4538-90a6-3415bb6b0e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e80749-2336-4f0c-a038-10053007ac1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
